language: jp
pipeline:
  - name: HFTransformersNLP
    model_weights: "BERT-base_mecab-ipadic-bpe-32k_whole-word-mask"
    model_name: "bert"
  - name: LanguageModelTokenizer
  - name: LanguageModelFeaturizer
  - name: DIETClassifier
    batch_strategy: sequence
    random_seed: 5
    intent_classification: True
    entity_recognition: False
    use_masked_language_model: True
    finetuning_task: True
    epochs: 200
    number_of_transformer_layers: 4
    transformer_size: 256
    drop_rate: 0.15
    weight_sparsity: 0.2
    batch_size: [64, 256]
    embedding_dimension: 500
    hidden_layer_sizes:
    text: [512, 128]


policies:
  - name: "FallbackPolicy"
    nlu_threshold: 0.3
    ambiguity_threshold: 0.1
    core_threshold: 0.3
    fallback_action_name: 'action_custom_fallback'
  - name: MemoizationPolicy
  - name: TEDPolicy
    max_history: 5
    epochs: 100