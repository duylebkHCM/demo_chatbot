language: jp
pipeline:
  - name: HFTransformersNLP
    model_weights: "BERT-base_mecab-ipadic-bpe-32k_whole-word-mask"
    model_name: "bert"
  - name: LanguageModelTokenizer
  - name: LanguageModelFeaturizer
  - name: DIETClassifier
    epochs: 200
    batch_strategy: sequence

policies:
  - name: "FallbackPolicy"
    nlu_threshold: 0.4
    core_threshold: 0.3
    fallback_action_name: 'action_custom_fallback'
  - name: MemoizationPolicy
  - name: TEDPolicy
    max_history: 5
    epochs: 100